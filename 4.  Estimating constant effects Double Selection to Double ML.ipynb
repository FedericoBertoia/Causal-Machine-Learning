{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"font-size: 40px; text-align: center;\">Causal Machine Learning Simulations</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"font-size: 20px; text-align: center;\">(Original material by Michael C. Knaus)</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <img src=\"https://pbs.twimg.com/media/FrGy6a5aMAAQ7u3?format=png&name=large\" alt=\"Alt text\" width=\"700\" height=\"350\">\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Estimating constant effects: Double Selection to Double ML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\text{Identifying Assumption 1 (Measured Confounding)}\n",
    "$$\n",
    "\n",
    "$$\n",
    "Y(w) \\perp\\!\\!\\perp W \\mid X \\quad \\text{for all} \\quad W \\in \\mathbb{W} \\subset \\mathbb{R}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\text{Modelling Assumption 1 (Linear Potential Outcomes)}\n",
    "$$\n",
    "\n",
    "$$\n",
    "Y(w) = \\tau W + X'\\beta + U_{Y(w)}; \\quad E[U_{Y(w)} \\mid X] = 0; \\quad \\forall w \\in \\mathbb{W}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.display import Image\n",
    "\n",
    "import graphviz as gr\n",
    "\n",
    "import statsmodels.api as sm\n",
    "\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.model_selection import cross_val_predict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Warnings\n",
    "\n",
    "import warnings\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Restore warnings\n",
    "warnings.filterwarnings(\"default\")\n",
    "\n",
    "import warnings\n",
    "\n",
    "# Suppress FutureWarnings related to is_sparse\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"sklearn.utils.validation\")\n",
    "\n",
    "# Reset the warnings filter if necessary\n",
    "# warnings.resetwarnings()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "Y(w) = 2W + X_1 + 0.5 X_2 + U_{Y(w)} \\quad \\text{(True model)}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "X_1 \\sim \\mathbb{U}(0,1)\n",
    "$$\n",
    "$$\n",
    "X_2 \\sim \\mathbb{U}(0,20)\n",
    "$$\n",
    "$$\n",
    "W \\sim 0.9 \\mathbb{U}(0,5) + 0.1 X_2\n",
    "$$\n",
    "$$\n",
    "U_{Y(w)} \\sim \\mathbb{N}(0,I_n)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DAGs to understand the problem\n",
    "\n",
    "g = gr.Digraph()\n",
    "\n",
    "g.edge(\"X1\", \"Y\")\n",
    "g.edge(\"X2\", \"Y\")\n",
    "g.edge(\"W\", \"Y\")\n",
    "g.edge(\"X2\", \"W\")\n",
    "g.node(\"X2\",\"X2\", color=\"red\")\n",
    "\n",
    "g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data generating process\n",
    "\n",
    "np.random.seed(21)\n",
    "\n",
    "def create_data(N=200):\n",
    "    df = pd.DataFrame({'X1': np.random.uniform(0, 1, size = N)})\n",
    "    df['X2'] = np.random.uniform(0, 20, size = N)\n",
    "    df['W'] = 0.9*np.random.uniform(0, 5, size=N) + 0.1*df['X2']\n",
    "    df['epsilon'] = np.random.normal(size = N)\n",
    "    df['Y'] = 2*df['W'] + df['X1'] + 0.5*df['X2'] + df['epsilon']\n",
    "    return df\n",
    "\n",
    "df = create_data(1000000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 OLS and omitted variable bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\hat{\\beta}^{OLS} = \\underset{\\beta}{\\text{arg min}} \\sum_{i=1}^{N} \\left(Y_i-\\beta_0-\\sum_{j=1}^p \\beta_j X_{ij} \\right)^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OLS \n",
    "\n",
    "Y = df['Y']\n",
    "X = sm.add_constant(df[['W','X1','X2']])\n",
    "\n",
    "\n",
    "model = sm.OLS(Y,X).fit()\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OLS (omitted variable bias)\n",
    "\n",
    "X = sm.add_constant(df[['W','X1']])\n",
    "\n",
    "model2 = sm.OLS(Y,X).fit()\n",
    "\n",
    "print(model2.summary())\n",
    "\n",
    "## Note that in this model, to get an unbiased estimate of X1 we need an higher sample size (why?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Double Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "Y(w) = 2W + X_1 + 0.5 X_2 - 0.05 X^2_2+ U_{Y(w)} \\quad \\text{(True model)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "X_1 \\sim \\mathbb{U}(0,1)\n",
    "$$\n",
    "$$\n",
    "X_2 \\sim \\mathbb{U}(0,20)\n",
    "$$\n",
    "$$\n",
    "W \\sim 0.8\\mathbb{U}(0,5) + 0.15 X_2 + 0.05 X^2_{2}\n",
    "$$\n",
    "$$\n",
    "U_{Y(w)} \\sim \\mathbb{N}(0,I_n)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data generating process\n",
    "\n",
    "np.random.seed(21)\n",
    "\n",
    "def create_data(N=200):\n",
    "    df = pd.DataFrame({'X1': np.random.uniform(0, 1, size = N)})\n",
    "    df['X2'] = np.random.uniform(0, 20, size = N)\n",
    "    df['W'] = 0.8*np.random.uniform(0, 5, size=N) + 0.15*df['X2'] + 0.05*df['X2']**2\n",
    "    df['epsilon'] = np.random.normal(size = N)\n",
    "    df['Y'] = 2*df['W'] + df['X1'] + 0.5*df['X2'] - 0.05*df['X2']**2 + df['epsilon']\n",
    "    return df\n",
    "\n",
    "df = create_data(100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OLS (now we get a biased estimate even if we include all the predictors)\n",
    "\n",
    "Y = df['Y']\n",
    "X = sm.add_constant(df[['W','X1','X2']])\n",
    "\n",
    "\n",
    "model = sm.OLS(Y,X).fit()\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Assume approximate sparsity ==> The number of controls with non-zero coefficients is small relative to the sample size\n",
    "1. Select variables that predict outcome via Post-Lasso (w/o treatment variable)\n",
    "2. Select variables that predict treatment via Post-Lasso\n",
    "3. Use union of selected variables w/ treatment variable in OLS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\text{Before applying LASSO, remember to scale the features!!}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\hat{\\beta}^{Lasso} = \\underset{\\beta}{\\text{arg min}} \\sum_{i=1}^{N} \\left(Y_i- \\beta_0-\\sum_{j=1}^p \\beta_j X_{ij} \\right)^2 + \\lambda \\sum_{j=1}^p \\lvert \\beta_j \\rvert\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\hat{\\beta}^{\\text{Post Lasso}} = \\underset{\\beta}{\\text{arg min}}\\sum_{i=1}^N \\left(Y_i - \\beta_0 - \\sum_{j=1}^s \\beta_j X^{sel}_{ij} \\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly = PolynomialFeatures(degree=2)\n",
    "\n",
    "X = df[['X1','X2']]\n",
    "scaler = StandardScaler()\n",
    "X_std = scaler.fit_transform(X)\n",
    "\n",
    "df_array = poly.fit_transform(X_std)\n",
    "\n",
    "feature_names = poly.get_feature_names_out(X.columns)\n",
    "\n",
    "df_poly = pd.DataFrame(df_array, columns= feature_names)\n",
    "\n",
    "df_poly.head()\n",
    "\n",
    "## Note, it is important to scale features before applying LASSO because the penalty term is related to the module of the coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = df['Y']\n",
    "\n",
    "X = df_poly[feature_names]\n",
    "\n",
    "model_outcome = LassoCV().fit(X,Y)\n",
    "\n",
    "alpha = model_outcome.alpha_\n",
    "\n",
    "coefficients = model_outcome.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_outcome.score(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"alpha:\", alpha)\n",
    "print(\"coefficients:\", coefficients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = df['W']\n",
    "\n",
    "model_treatment = LassoCV().fit(X,W)\n",
    "\n",
    "alpha = model_treatment.alpha_\n",
    "\n",
    "coefficients = model_treatment.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"alpha:\", alpha)\n",
    "print(\"coefficients:\", coefficients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = df['Y']\n",
    "\n",
    "X = pd.DataFrame({\n",
    "    'W': df['W'],\n",
    "    'X2^2': df_poly['X2^2'],\n",
    "    'X2': df['X2']\n",
    "})\n",
    "\n",
    "X = sm.add_constant(X)\n",
    "\n",
    "model_ds = sm.OLS(Y,X).fit()\n",
    "\n",
    "print(model_ds.summary())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Double ML: Partially Linear Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "Y(w) = 2W + X_1 + 0.5 e^{X_2} + U_{Y(w)} \\quad \\text{(True model)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "X_1 \\sim \\mathbb{U}(0,1)\n",
    "$$\n",
    "$$\n",
    "X_2 \\sim \\mathbb{U}(0,5)\n",
    "$$\n",
    "$$\n",
    "W \\sim \\mathbb{U}(0,5) + 0.1e^{X_2}\n",
    "$$\n",
    "$$\n",
    "U_{Y(w)} \\sim \\mathbb{N}(0,I_n)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(21)\n",
    "\n",
    "def create_data(N=200):\n",
    "    df = pd.DataFrame({'X1': np.random.uniform(0, 1, size = N)})\n",
    "    df['X2'] = np.random.uniform(0, 5, size = N)\n",
    "    df['W'] = np.random.uniform(0, 5, size=N) + 0.1*np.exp(df['X2'])  \n",
    "    df['epsilon'] = np.random.normal(size = N)\n",
    "    df['Y'] = 2*df['W'] + df['X1']  + 0.5* np.exp(df['X2']) + df['epsilon']  \n",
    "    return df\n",
    "\n",
    "df = create_data(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = df['Y']\n",
    "X = sm.add_constant(df[['W','X1','X2']])\n",
    "\n",
    "\n",
    "model = sm.OLS(Y,X).fit()\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\text{Modelling Assumption 2 (Partially Linear Potential Outcomes)}\n",
    "$$\n",
    "\n",
    "$$\n",
    "Y(w) = \\tau W + g(X) + U_{Y(w)}; \\quad E[U_{Y(w)} \\mid X] = 0; \\quad \\forall w \\in \\mathbb{W}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "  \\text{1. Form prediction model for the treatment:} \\quad \\hat{e}(X) \\\\ \n",
    "  \\text{2. Form prediction model for the outcome:} \\quad \\hat{m}(X) \\\\\n",
    "  \\text{3. Run feasible residual-on-residual regression:} \\quad \\hat{\\tau} = \\underset{\\tau}{\\text{arg min}} \\frac{1}{N} \\sum_{i=1}^N \\big(Y_i - \\hat{m}(X_i)- \\tau(W_i-\\hat{e}(X_i))\\big)^2 \n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Image\n",
    "\n",
    "# Define file paths for the images\n",
    "file_paths = [\n",
    "    r\"C:\\Users\\feder\\Desktop\\Progetti Extra ML\\Causal Machine Learning\\4.  Folder Estimating constant effects Double Selection to Double ML\\DML partially linear 1.png\",\n",
    "    r\"C:\\Users\\feder\\Desktop\\Progetti Extra ML\\Causal Machine Learning\\4.  Folder Estimating constant effects Double Selection to Double ML\\DML partially linear 2.png\"\n",
    "]\n",
    "\n",
    "# Define width and height for each image\n",
    "width = 1000\n",
    "height = 600\n",
    "\n",
    "# Create a list to store Image objects\n",
    "images = []\n",
    "\n",
    "# Load and display each image\n",
    "for file_path in file_paths:\n",
    "    image = Image(filename=file_path, width=width, height=height)\n",
    "    images.append(image)\n",
    "\n",
    "# Display images side by side\n",
    "display(*images)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict E[W|X]\n",
    "random_forest = RandomForestRegressor()\n",
    "\n",
    "W = df['W']\n",
    "X = df[['X1','X2']]\n",
    "\n",
    "e_hat = cross_val_predict(random_forest, X, W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict E[Y|X]\n",
    "Y = df['Y']\n",
    "X = df[['X1','X2']]\n",
    "\n",
    "m_hat = cross_val_predict(random_forest, X, Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_res = Y - m_hat\n",
    "w_res = W - e_hat\n",
    "\n",
    "model_double_ML_partially_linear = sm.OLS(y_res,w_res).fit()\n",
    "\n",
    "print(model_double_ML_partially_linear.summary())\n",
    "\n",
    "## Notice that we don't have the constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
